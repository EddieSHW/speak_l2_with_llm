import gradio as gr
import requests
import json
import os
import tempfile
import time
from gtts import gTTS
import speech_recognition as sr
from pydub import AudioSegment
from pydub.playback import play
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# Ollama APIè¨­å®š
OLLAMA_API_URL = os.getenv("OLLAMA_API_URL", "http://localhost:11434")
MODEL_NAME = os.getenv("MODEL_NAME", "gemma3")

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´
chat_history = []

# éŸ³å£°èªè­˜é–¢æ•°
def transcribe_audio(audio_file):
    try:
        recognizer = sr.Recognizer()
        with sr.AudioFile(audio_file) as source:
            audio_data = recognizer.record(source)
            text = recognizer.recognize_google(audio_data, language="ja-JP")
            return text
    except Exception as e:
        return f"éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: {str(e)}"

# ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿ä¸Šã’é–¢æ•°
def text_to_speech(text):
    try:
        tts = gTTS(text=text, lang='ja', slow=False)
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
        temp_file.close()
        tts.save(temp_file.name)
        return temp_file.name
    except Exception as e:
        print(f"éŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

# Ollama APIã§ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚’å–å¾—
def get_ollama_response(message, history, temperature=0.7, max_tokens=2048):
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’æ•´å½¢
    messages = []
    for human, ai in history:
        messages.append({"role": "user", "content": human})
        if ai:  # AIã®å¿œç­”ãŒã‚ã‚‹å ´åˆ
            messages.append({"role": "assistant", "content": ai})
    
    # æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
    messages.append({"role": "user", "content": message})
    
    # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æº–å‚™
    data = {
        "model": MODEL_NAME,
        "messages": messages,
        "stream": False,
        "options": {
            "temperature": temperature,
            "num_predict": max_tokens
        }
    }
    
    try:
        response = requests.post(f"{OLLAMA_API_URL}/api/chat", json=data)
        if response.status_code != 200:
            return f"ã‚¨ãƒ©ãƒ¼: {response.status_code} - {response.text}"
        
        response_data = response.json()
        return response_data["message"]["content"]
    except Exception as e:
        return f"APIã‚¨ãƒ©ãƒ¼: {str(e)}"

# ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã«ã‚ˆã‚‹ãƒãƒ£ãƒƒãƒˆå‡¦ç†
def chat(message, history, temperature, max_tokens):
    # Ollamaã‹ã‚‰ã®å¿œç­”ã‚’å–å¾—
    response = get_ollama_response(message, history, temperature, max_tokens)
    
    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆ
    audio_file = text_to_speech(response)
    
    # å±¥æ­´ã‚’æ›´æ–°ã—ã¦è¿”ã™
    history.append((message, response))
    return history, history, audio_file

# éŸ³å£°å…¥åŠ›ã«ã‚ˆã‚‹ãƒãƒ£ãƒƒãƒˆå‡¦ç†
def voice_chat(audio_file, history, temperature, max_tokens):
    if audio_file is None:
        return history, history, None
    
    # éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
    text = transcribe_audio(audio_file)
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å¿œç­”ã‚’ç”Ÿæˆ
    response = get_ollama_response(text, history, temperature, max_tokens)
    
    # å¿œç­”ã‚’éŸ³å£°ã«å¤‰æ›
    audio_response = text_to_speech(response)
    
    # å±¥æ­´ã‚’æ›´æ–°
    history.append((text, response))
    return history, history, audio_response

# åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
def get_available_models():
    try:
        response = requests.get(f"{OLLAMA_API_URL}/api/tags")
        if response.status_code == 200:
            models = response.json().get("models", [])
            return [m["name"] for m in models]
        return []
    except:
        return []

# Gradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®æ§‹ç¯‰
def build_interface():
    with gr.Blocks(title="Gemma3 æ—¥æœ¬èªéŸ³å£°ãƒãƒ£ãƒƒãƒˆ") as demo:
        gr.Markdown("# ğŸ¤– Gemma3 æ—¥æœ¬èªéŸ³å£°ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
        gr.Markdown("Ollamaã‚’ä½¿ç”¨ã—ã¦LLMã¨æ—¥æœ¬èªã§ä¼šè©±ã§ãã¾ã™ã€‚éŸ³å£°å…¥åŠ›ã¨éŸ³å£°å‡ºåŠ›ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚")

        with gr.Row():
            with gr.Column(scale=3):
                chatbot = gr.Chatbot(label="ä¼šè©±", height=500)
                
                with gr.Row():
                    text_input = gr.Textbox(label="ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›", placeholder="ã“ã“ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›...", lines=2)
                
                with gr.Row():
                    audio_input = gr.Audio(source="microphone", type="filepath", label="éŸ³å£°ã§ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›")
                
                with gr.Row():
                    audio_output = gr.Audio(label="AIã®å¿œç­”ï¼ˆéŸ³å£°ï¼‰", autoplay=True)
                
                with gr.Row():
                    clear_btn = gr.Button("ä¼šè©±ã‚’ã‚¯ãƒªã‚¢")
                
            with gr.Column(scale=1):
                available_models = get_available_models()
                default_model = MODEL_NAME if MODEL_NAME in available_models else (available_models[0] if available_models else MODEL_NAME)
                
                model_dropdown = gr.Dropdown(
                    choices=available_models if available_models else [MODEL_NAME],
                    value=default_model,
                    label="ãƒ¢ãƒ‡ãƒ«é¸æŠ"
                )
                
                temperature = gr.Slider(
                    minimum=0.0,
                    maximum=2.0,
                    value=0.7,
                    step=0.1,
                    label="Temperature"
                )
                
                max_tokens = gr.Slider(
                    minimum=16,
                    maximum=4096,
                    value=2048,
                    step=16,
                    label="æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°"
                )
                
                gr.Markdown("### ãƒ¢ãƒ‡ãƒ«æƒ…å ±")
                if available_models:
                    gr.Markdown("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«:")
                    for model in available_models:
                        gr.Markdown(f"- `{model}`")
                else:
                    gr.Markdown("**è­¦å‘Š**: Ollamaã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ããªã„ã‹ã€ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                    gr.Markdown("OllamaãŒå®Ÿè¡Œä¸­ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ã®è¨­å®š
        text_input.submit(chat, [text_input, chatbot, temperature, max_tokens], [chatbot, chatbot, audio_output]).then(
            lambda: "", None, [text_input]
        )
        
        audio_input.change(voice_chat, [audio_input, chatbot, temperature, max_tokens], [chatbot, chatbot, audio_output])
        
        clear_btn.click(lambda: [], None, [chatbot])
        
        # ãƒ¢ãƒ‡ãƒ«å¤‰æ›´æ™‚ã®å‡¦ç†
        def update_model(model_name):
            global MODEL_NAME
            MODEL_NAME = model_name
            return MODEL_NAME
        
        model_dropdown.change(update_model, [model_dropdown], None)
        
        # ãƒ•ãƒƒã‚¿ãƒ¼
        gr.Markdown("---")
        gr.Markdown("*ã“ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ãƒ­ãƒ¼ã‚«ãƒ«ã®Ollamaã‚’ä½¿ç”¨ã—ã¦LLMã¨ä¼šè©±ã—ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã¯ã‚ãªãŸã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‹ã‚‰å¤–éƒ¨ã«é€ä¿¡ã•ã‚Œã¾ã›ã‚“ã€‚*")
    
    return demo

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
if __name__ == "__main__":
    demo = build_interface()
    demo.launch(share=False) 